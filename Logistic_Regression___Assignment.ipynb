{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?   \n",
        "Answer- Logistic regression is a supervised machine learning algorithm used for classification problems. Instead of predicting a continuous numerical value like linear regression, it predicts the probability of a categorical outcome, such as \"yes\" or \"no,\" or \"spam\" or \"not spam.\" It does this by using a sigmoid function (also known as the logistic function) to transform the output of a linear equation into a value between 0 and 1, which can be interpreted as a probability.\n",
        "\n",
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression.  \n",
        "Answer- The Sigmoid function is a crucial component of logistic regression because it maps the output of the linear model to a probability value between 0 and 1. This is essential for classification problems, where the goal is to predict a categorical outcome.\n",
        "\n",
        "Here's a breakdown of its role:\n",
        "\n",
        "Transforms Linear Output: The linear regression part of the logistic regression model can produce any real-valued number, from negative infinity to positive infinity. Since probabilities must be between 0 and 1, the raw linear output cannot be used directly. The Sigmoid function \"squashes\" or transforms this output into the desired range.\n",
        "\n",
        "\n",
        "Interprets as Probability: The S-shaped curve of the Sigmoid function ensures that the output value can be interpreted as a probability. A value close to 1 indicates a high probability of belonging to one class (e.g., \"yes\" or \"spam\"), while a value close to 0 indicates a high probability of belonging to the other class.\n",
        "\n",
        "\n",
        "Enables Classification: By using a threshold (typically 0.5), the probability value from the Sigmoid function is converted into a final class prediction. If the probability is above the threshold, the model classifies the input as one class; if it's below, it classifies it as the other.    \n",
        "\n",
        "Question 3: What is Regularization in Logistic Regression and why is it needed?  \n",
        "Answer- Regularization in logistic regression is a technique used to prevent overfitting, which occurs when a model learns the training data too well, including its noise and outliers, and consequently performs poorly on new, unseen data.\n",
        "\n",
        "The core idea is to add a penalty term to the loss function that the model is trying to minimize. This penalty discourages the model from assigning excessively large weights (coefficients) to the features.\n",
        "\n",
        "\n",
        "Why It's Needed\n",
        "To Prevent Overfitting: Without regularization, a logistic regression model with many features may assign very high weights to noisy or irrelevant features to perfectly fit the training data. This makes the model overly complex and less able to generalize to new data. The penalty term forces the model to be simpler and more robust.\n",
        "\n",
        "\n",
        "To Handle Multicollinearity: Regularization can also help when features are highly correlated (a problem known as multicollinearity). In such cases, the model's weights can become unstable and very large. Regularization stabilizes these weights by penalizing their magnitude.\n",
        "\n",
        "\n",
        "There are two main types of regularization used in logistic regression:\n",
        "\n",
        "L1 Regularization (Lasso): Adds a penalty term that is the sum of the absolute values of the weights. This can force the weights of less important features to become exactly zero, effectively performing feature selection.\n",
        "\n",
        "\n",
        "L2 Regularization (Ridge): Adds a penalty term that is the sum of the squared values of the weights. This forces weights to be small but rarely exactly zero. It's often preferred when all features are relevant and you want to keep them.  \n",
        "\n",
        "Question 4: What are some common evaluation metrics for classification models, and\n",
        "why are they important?    \n",
        "Answer- Evaluating classification models is crucial because a single metric like accuracy can be misleading, especially with imbalanced datasets. For example, a model predicting a rare disease will have high accuracy just by always predicting \"no disease,\" which is not useful. Therefore, a combination of metrics is needed to provide a comprehensive view of the model's performance.\n",
        "\n",
        "\n",
        "Key Evaluation Metrics\n",
        "1. Accuracy\n",
        "Accuracy is the proportion of correct predictions out of all predictions. It's the most intuitive metric but can be misleading for imbalanced datasets.\n",
        "\n",
        "\n",
        "\n",
        "Accuracy=\n",
        "Total Predictions\n",
        "True Positives (TP)+True Negatives (TN)\n",
        "​\n",
        "\n",
        "2. Precision\n",
        "Precision measures the proportion of positive predictions that were actually correct. It's important when the cost of a false positive is high (e.g., a spam filter flagging a legitimate email as spam).\n",
        "\n",
        "\n",
        "Precision=\n",
        "True Positives (TP)+False Positives (FP)\n",
        "True Positives (TP)\n",
        "​\n",
        "\n",
        "3. Recall (Sensitivity)\n",
        "Recall measures the proportion of actual positives that were correctly identified. It's important when the cost of a false negative is high (e.g., a medical test failing to detect a disease).\n",
        "\n",
        "\n",
        "Recall=\n",
        "True Positives (TP)+False Negatives (FN)\n",
        "True Positives (TP)\n",
        "​\n",
        "\n",
        "4. F1-Score\n",
        "The F1-Score is the harmonic mean of precision and recall. It provides a single score that balances both metrics and is particularly useful for imbalanced datasets.\n",
        "\n",
        "\n",
        "F1-Score=2×\n",
        "Precision+Recall\n",
        "Precision×Recall\n",
        "​\n",
        "\n",
        "5. AUC-ROC Curve\n",
        "The Area Under the Curve of the Receiver Operating Characteristic curve plots the True Positive Rate (Recall) against the False Positive Rate at different classification thresholds.  The AUC value (ranging from 0 to 1) summarizes the model's ability to distinguish between the positive and negative classes across all possible thresholds, making it a robust metric for imbalanced data.   \n",
        "\n",
        "Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)  \n",
        "Answer-\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xna6oXSmfTj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a dataset from scikit-learn and convert to a pandas DataFrame\n",
        "# We'll use the breast cancer dataset, which is a binary classification problem\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "# The dataset has already been split into features (X) and target (y)\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(\"Number of features:\", X.shape[1])\n",
        "print(\"Number of samples:\", X.shape[0])\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "# We'll use 80% for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nData split into training and testing sets.\")\n",
        "print(f\"Training set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "# Step 3: Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nLogistic Regression model trained successfully!\")\n",
        "\n",
        "# Step 4: Make predictions and evaluate the model's accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX8DTK6Ng55C",
        "outputId": "a9b91ba3-b4da-4b18-b17b-539384a80ad8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Number of features: 30\n",
            "Number of samples: 569\n",
            "\n",
            "Data split into training and testing sets.\n",
            "Training set size: 455 samples\n",
            "Testing set size: 114 samples\n",
            "\n",
            "Logistic Regression model trained successfully!\n",
            "\n",
            "Model Accuracy: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.    \n",
        "Answer-\n"
      ],
      "metadata": {
        "id": "EEJMV3a_hCVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a dataset (e.g., the breast cancer dataset)\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "print(\"Dataset loaded successfully!\")\n",
        "\n",
        "# Step 2: Split the data into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Data split into training and testing sets.\")\n",
        "\n",
        "# Step 3: Initialize and train a Logistic Regression model with L2 regularization\n",
        "# The 'penalty' parameter is set to 'l2' for Ridge regularization.\n",
        "# 'C' is the inverse of the regularization strength (smaller C means stronger regularization).\n",
        "# 'max_iter' is increased to ensure convergence for this dataset.\n",
        "model = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "print(\"\\nLogistic Regression model with L2 regularization trained successfully!\")\n",
        "\n",
        "# Step 4: Print the model coefficients\n",
        "# The coefficients show the weight assigned to each feature by the trained model.\n",
        "print(\"\\nModel Coefficients (Weights):\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Step 5: Make predictions and evaluate the model's accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy on Test Set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF7tbGAjhWRz",
        "outputId": "0ec316df-5b0f-4e5b-f42d-d83aa81a1662"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n",
            "Data split into training and testing sets.\n",
            "\n",
            "Logistic Regression model with L2 regularization trained successfully!\n",
            "\n",
            "Model Coefficients (Weights):\n",
            "mean radius: 2.1325\n",
            "mean texture: 0.1528\n",
            "mean perimeter: -0.1451\n",
            "mean area: -0.0008\n",
            "mean smoothness: -0.1426\n",
            "mean compactness: -0.4156\n",
            "mean concavity: -0.6519\n",
            "mean concave points: -0.3445\n",
            "mean symmetry: -0.2076\n",
            "mean fractal dimension: -0.0298\n",
            "radius error: -0.0500\n",
            "texture error: 1.4430\n",
            "perimeter error: -0.3039\n",
            "area error: -0.0726\n",
            "smoothness error: -0.0162\n",
            "compactness error: -0.0019\n",
            "concavity error: -0.0449\n",
            "concave points error: -0.0377\n",
            "symmetry error: -0.0418\n",
            "fractal dimension error: 0.0056\n",
            "worst radius: 1.2321\n",
            "worst texture: -0.4046\n",
            "worst perimeter: -0.0362\n",
            "worst area: -0.0271\n",
            "worst smoothness: -0.2626\n",
            "worst compactness: -1.2090\n",
            "worst concavity: -1.6180\n",
            "worst concave points: -0.6153\n",
            "worst symmetry: -0.7428\n",
            "worst fractal dimension: -0.1170\n",
            "\n",
            "Model Accuracy on Test Set: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)    \n",
        "Answer-"
      ],
      "metadata": {
        "id": "w8SLfG-_hfyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load a multiclass dataset from scikit-learn\n",
        "# The Iris dataset is a classic example with 3 classes.\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target, name='species')\n",
        "print(\"Iris dataset loaded successfully!\")\n",
        "print(\"Number of features:\", X.shape[1])\n",
        "print(\"Number of samples:\", X.shape[0])\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"\\nData split into training and testing sets.\")\n",
        "print(f\"Training set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "# Step 3: Initialize and train the Logistic Regression model for multiclass classification\n",
        "# We set multi_class='ovr' to use the One-vs-Rest strategy.\n",
        "# The 'liblinear' solver is a good choice for 'ovr'.\n",
        "# max_iter is increased to ensure convergence.\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"\\nLogistic Regression model with 'ovr' trained successfully!\")\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgY6ldDehvVB",
        "outputId": "98929918-1c35-4b95-dea0-85bb81b1b3e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset loaded successfully!\n",
            "Number of features: 4\n",
            "Number of samples: 150\n",
            "\n",
            "Data split into training and testing sets.\n",
            "Training set size: 120 samples\n",
            "Testing set size: 30 samples\n",
            "\n",
            "Logistic Regression model with 'ovr' trained successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.   \n",
        "Answer-\n"
      ],
      "metadata": {
        "id": "3yC-8asthyTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load a multiclass dataset from scikit-learn\n",
        "# The Iris dataset is a classic example with 3 classes.\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target, name='species')\n",
        "print(\"Iris dataset loaded successfully!\")\n",
        "print(\"Number of features:\", X.shape[1])\n",
        "print(\"Number of samples:\", X.shape[0])\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"\\nData split into training and testing sets.\")\n",
        "print(f\"Training set size: {len(X_train)} samples\")\n",
        "print(f\"Testing set size: {len(X_test)} samples\")\n",
        "\n",
        "# Step 3: Initialize and train the Logistic Regression model for multiclass classification\n",
        "# We set multi_class='ovr' to use the One-vs-Rest strategy.\n",
        "# The 'liblinear' solver is a good choice for 'ovr'.\n",
        "# max_iter is increased to ensure convergence.\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"\\nLogistic Regression model with 'ovr' trained successfully!\")\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the classification report\n",
        "# The classification report provides precision, recall, f1-score, and support for each class.\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyqw8gX-h7PV",
        "outputId": "d9c16048-3def-40ac-9e77-ac100e3dab7d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataset loaded successfully!\n",
            "Number of features: 4\n",
            "Number of samples: 150\n",
            "\n",
            "Data split into training and testing sets.\n",
            "Training set size: 120 samples\n",
            "Testing set size: 30 samples\n",
            "\n",
            "Logistic Regression model with 'ovr' trained successfully!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n"
      ],
      "metadata": {
        "id": "QMBWkspDh-sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a dataset with features of different scales\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a model without scaling\n",
        "model_unscaled = LogisticRegression(max_iter=1000)\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred_unscaled = model_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "print(\"Accuracy WITHOUT Feature Scaling: {:.4f}\".format(accuracy_unscaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw7cXPLdiEhy",
        "outputId": "bc7060c1-8103-40de-813d-50ae1a407f60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Feature Scaling: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the TRAINING data and transform both training and testing data\n",
        "# This is crucial to prevent data leakage from the test set.\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a new model on the scaled data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy WITH Feature Scaling: {:.4f}\".format(accuracy_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLYamEU7iVk0",
        "outputId": "2825bf5f-1947-4323-e02a-c00bd91d325c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITH Feature Scaling: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.     \n",
        "Amswer- To build a robust Logistic Regression model for predicting customer response to a marketing campaign with an imbalanced dataset, I would follow a structured approach focusing on data handling, model training, and evaluation tailored to the business problem.\n",
        "\n",
        "1. Data Handling & Preprocessing\n",
        "Load and Clean Data: First, I would load the customer data, which includes features like purchase history, browsing behavior, demographics, and the target variable (responded/not responded). I would handle missing values by either imputing them or removing rows/columns, and correct any data entry errors.\n",
        "\n",
        "Feature Scaling: Since Logistic Regression is sensitive to the scale of features, I would use StandardScaler to standardize all numerical features. This ensures that features like total_spend and last_login_days are on a similar scale, preventing features with larger values from dominating the model.\n",
        "\n",
        "2. Handling Imbalanced Classes\n",
        "Given that only 5% of customers respond, the dataset is highly imbalanced. Training a model on this data directly would likely result in it always predicting the majority class (\"no response\") and achieving a misleadingly high accuracy. To address this, I would use Oversampling on the minority class. Specifically, I would use the SMOTE (Synthetic Minority Over-sampling Technique) algorithm. SMOTE generates synthetic data points for the minority class, effectively balancing the dataset without simply duplicating existing data.\n",
        "\n",
        "3. Model Training & Hyperparameter Tuning\n",
        "Split the Data: I would split the dataset into a training set and a testing set (e.g., 80/20 split) after applying SMOTE to the training data. This is crucial to prevent data leakage and ensure the model is evaluated on unseen, imbalanced data.\n",
        "\n",
        "Hyperparameter Tuning: Logistic Regression has key hyperparameters that can be tuned, such as:\n",
        "\n",
        "C: The regularization strength (inverse of lambda). A smaller value of C leads to stronger regularization. I would use a range of C values to find the optimal trade-off between bias and variance.\n",
        "\n",
        "penalty: The type of regularization (l1, l2, elasticnet). I would try L1 regularization to potentially perform feature selection and L2 for its ability to prevent large coefficient values.\n",
        "\n",
        "I would use a technique like Grid Search with Cross-Validation to systematically test different combinations of these hyperparameters and select the model that performs best on the validation sets.\n",
        "\n",
        "4. Model Evaluation & Business Impact\n",
        "Since accuracy is not a reliable metric for this imbalanced dataset, I would focus on metrics that are more relevant to the business goal.\n",
        "\n",
        "Precision and Recall: For this use case, recall is a very important metric because we want to identify as many of the actual responding customers as possible to avoid missing out on potential conversions. Precision is also important to ensure we are not spending marketing resources on customers who are unlikely to respond. A high-precision model would be efficient, while a high-recall model would be comprehensive.\n",
        "\n",
        "F1-Score: The F1-Score provides a single value that represents the harmonic mean of precision and recall, offering a good balance between the two.\n",
        "\n",
        "AUC-ROC Curve: The Area Under the ROC Curve (AUC) is an excellent metric for imbalanced data. It measures the model's ability to discriminate between positive and negative classes across various probability thresholds. A higher AUC value indicates a better model.\n",
        "\n",
        "Business-Specific Metrics: I would also evaluate the model based on business metrics like Lift and ROI (Return on Investment). The Lift chart would show how much more likely the top N% of customers predicted by the model are to respond compared to a random sample. This directly translates to the business value of the model.\n",
        "\n",
        "By using a combination of these metrics, I can present a clear picture of the model's effectiveness to the e-commerce team, demonstrating not just its technical performance but its potential business impact.\n"
      ],
      "metadata": {
        "id": "vBnpQfsyiamk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yyneRfMLi3Pb"
      }
    }
  ]
}